## nlp-learning
### 语言模型
+ 统计语言模型（Statistical Language models，SLM）<br>
+ 神经语言模型（Neural Language Models，NLM）<br>
+ 预训练语言模型（Pre-trained Language Models，PLM）<br>
+ 大规模语言模型（Large Language models，SLM）<br>

+ 教程<br>
[动手深度学习v2](https://zh.d2l.ai/chapter_attention-mechanisms/index.html)<br>
[ReadPaper-论文总结平台](https://readpaper.com/paper/2963403868)

+ 论文翻译<br>
[Attention Is All You Need](https://www.yiyibooks.cn/yiyibooks/Attention_Is_All_You_Need/index.html)<br>
[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://yiyibooks.cn/nlp/bert/main.html)<br>
[ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS](https://yiyibooks.cn/yiyibooks/A_LITE_BERT_FOR_SELFSUPERVISED_LEARNING_OF_LANGUAGE_REPRESENTATIONS/index.html)
[自然语言处理中的注意力机制研究综述*](https://manu44.magtech.com.cn/Jwk_infotech_wk3/article/2020/2096-3467/2096-3467-4-5-1.shtml)
